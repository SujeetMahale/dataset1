{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a0dd84",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a52591",
   "metadata": {},
   "source": [
    "# Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39b0ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096b5ed",
   "metadata": {},
   "source": [
    "# Question no 01"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57a2585f",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21546eac",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a60d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e17101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect to the driver-\n",
    "\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# entering designation and location as required in the question-\n",
    "\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Analyst\")\n",
    "\n",
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c71352a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title from the given page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "# scraping job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "# scraping job experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 expwdth\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52e97073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SERIAL_NUM</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>JOB_LOCATION</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EXP_REQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Project Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>WSP</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Data Analyst, Business Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(HSR Layout +1)</td>\n",
       "      <td>Clinilaunch Research Institute Llp</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Temp. WFH - Bangalore/Bengaluru</td>\n",
       "      <td>Treebo Hotels</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Executive - Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cargill</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Eli Lilly And Company</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SERIAL_NUM                       JOB_TITLE  \\\n",
       "0           1                    Data Analyst   \n",
       "1           2                    Data Analyst   \n",
       "2           3                    Data Analyst   \n",
       "3           4          Senior Data Analyst II   \n",
       "4           5            Project Data Analyst   \n",
       "5           6  Data Analyst, Business Analyst   \n",
       "6           7                    Data Analyst   \n",
       "7           8        Executive - Data Analyst   \n",
       "8           9                    Data Analyst   \n",
       "9          10                    Data Analyst   \n",
       "\n",
       "                         JOB_LOCATION                        COMPANY_NAME  \\\n",
       "0                 Bangalore/Bengaluru                       Shell Pvt Ltd   \n",
       "1                 Bangalore/Bengaluru                       Shell Pvt Ltd   \n",
       "2                 Bangalore/Bengaluru                       Shell Pvt Ltd   \n",
       "3                 Bangalore/Bengaluru                            Flipkart   \n",
       "4                 Bangalore/Bengaluru                                 WSP   \n",
       "5  Bangalore/Bengaluru(HSR Layout +1)  Clinilaunch Research Institute Llp   \n",
       "6     Temp. WFH - Bangalore/Bengaluru                       Treebo Hotels   \n",
       "7                 Bangalore/Bengaluru                            Flipkart   \n",
       "8                 Bangalore/Bengaluru                             Cargill   \n",
       "9                 Bangalore/Bengaluru               Eli Lilly And Company   \n",
       "\n",
       "   EXP_REQ  \n",
       "0  2-5 Yrs  \n",
       "1  3-5 Yrs  \n",
       "2  2-5 Yrs  \n",
       "3  2-4 Yrs  \n",
       "4  0-4 Yrs  \n",
       "5  0-5 Yrs  \n",
       "6  1-5 Yrs  \n",
       "7  1-5 Yrs  \n",
       "8  2-4 Yrs  \n",
       "9  2-6 Yrs  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'SERIAL_NUM':range(1,11,1),'JOB_TITLE':job_title,'JOB_LOCATION':job_location,'COMPANY_NAME':company_name,'EXP_REQ':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491cfec",
   "metadata": {},
   "source": [
    "# Question no 02"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90d09ac0",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a3a6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b04d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation and location as required in the question-\n",
    "\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20851973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title from the given page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "# scraping job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed2bc0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SERIAL_NUM</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>JOB_LOCATION</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...</td>\n",
       "      <td>Tata Nexarc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Nagpur, Pune</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Data &amp; Analytics Lead, Geo Analytics - GAMMA</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Weather and Climate Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...</td>\n",
       "      <td>Mindtree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Data Scientist - II</td>\n",
       "      <td>Bangalore/Bengaluru, India, Mumbai (All Areas)</td>\n",
       "      <td>Bizongo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>Baker Hughes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Manager-Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>AMERICAN EXPRESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>ACN - Applied Intelligence - Data Scientist - 09</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SERIAL_NUM                                         JOB_TITLE  \\\n",
       "0           1                  Analystics & Modeling Specialist   \n",
       "1           2                                    Data Scientist   \n",
       "2           3                                    Data Scientist   \n",
       "3           4      Data & Analytics Lead, Geo Analytics - GAMMA   \n",
       "4           5                Weather and Climate Data Scientist   \n",
       "5           6                                    Data Scientist   \n",
       "6           7                               Data Scientist - II   \n",
       "7           8                             Senior Data Scientist   \n",
       "8           9                              Manager-Data Science   \n",
       "9          10  ACN - Applied Intelligence - Data Scientist - 09   \n",
       "\n",
       "                                        JOB_LOCATION             COMPANY_NAME  \n",
       "0  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...                Accenture  \n",
       "1  Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...              Tata Nexarc  \n",
       "2                  Bangalore/Bengaluru, Nagpur, Pune            Tech Mahindra  \n",
       "3                                Bangalore/Bengaluru  Boston Consulting Group  \n",
       "4                                Bangalore/Bengaluru            Shell Pvt Ltd  \n",
       "5  Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...                 Mindtree  \n",
       "6     Bangalore/Bengaluru, India, Mumbai (All Areas)                  Bizongo  \n",
       "7                        Bangalore/Bengaluru, Mumbai             Baker Hughes  \n",
       "8                                Bangalore/Bengaluru         AMERICAN EXPRESS  \n",
       "9                                Bangalore/Bengaluru                Accenture  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'SERIAL_NUM':range(1,11,1),'JOB_TITLE':job_title,'JOB_LOCATION':job_location,'COMPANY_NAME':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b9b11f",
   "metadata": {},
   "source": [
    "# Question no 03"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bdad824",
   "metadata": {},
   "source": [
    "In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "ASSIGNMENT 2\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0f64675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b874f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation and location as required in the question-\n",
    "\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "539fa7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfa214ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32617b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title from the given page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "# scraping job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "# scraping job experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 expwdth\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46b34c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SERIAL_NUM</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>JOB_LOCATION</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EXP_REQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Artificial Intelligence/Computer Vision Engine...</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Vicara</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Data Activation Specialist - Adobe Target</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Okda Solutions</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Temp. WFH - Noida</td>\n",
       "      <td>NGI Ventures</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Opening For Jr. Data Scientist with Tatras Dat...</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>Tatras Data Services</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...</td>\n",
       "      <td>torcai digital media</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Americana Restaurants (india)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>URGENT: Data Scientist | Gurugram | 5 Days Wor...</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Digilytics</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Alliance Recruitment Agency</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>SE/SSE-Data Scientist</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>Bold Technology Systems</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SERIAL_NUM                                          JOB_TITLE  \\\n",
       "0           1  Artificial Intelligence/Computer Vision Engine...   \n",
       "1           2          Data Activation Specialist - Adobe Target   \n",
       "2           3                  Data Scientist - Engine Algorithm   \n",
       "3           4                                     Data Scientist   \n",
       "4           5  Opening For Jr. Data Scientist with Tatras Dat...   \n",
       "5           6                                     Data Scientist   \n",
       "6           7                                     Data Scientist   \n",
       "7           8  URGENT: Data Scientist | Gurugram | 5 Days Wor...   \n",
       "8           9                                     Data Scientist   \n",
       "9          10                              SE/SSE-Data Scientist   \n",
       "\n",
       "                                        JOB_LOCATION  \\\n",
       "0  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "1  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "2  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "3                                  Temp. WFH - Noida   \n",
       "4                                        Delhi / NCR   \n",
       "5  Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...   \n",
       "6                                   Gurgaon/Gurugram   \n",
       "7                                   Gurgaon/Gurugram   \n",
       "8                                              Noida   \n",
       "9                                        Delhi / NCR   \n",
       "\n",
       "                    COMPANY_NAME   EXP_REQ  \n",
       "0                         Vicara   1-3 Yrs  \n",
       "1                 Okda Solutions  7-10 Yrs  \n",
       "2                   Primo Hiring   1-3 Yrs  \n",
       "3                   NGI Ventures   1-5 Yrs  \n",
       "4           Tatras Data Services   2-4 Yrs  \n",
       "5           torcai digital media   2-7 Yrs  \n",
       "6  Americana Restaurants (india)   3-8 Yrs  \n",
       "7                     Digilytics   2-5 Yrs  \n",
       "8    Alliance Recruitment Agency   3-4 Yrs  \n",
       "9        Bold Technology Systems   3-8 Yrs  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'SERIAL_NUM':range(1,11,1),'JOB_TITLE':job_title,'JOB_LOCATION':job_location,'COMPANY_NAME':company_name,'EXP_REQ':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71115d",
   "metadata": {},
   "source": [
    "# Question no 04"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34be907e",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "236bad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b385f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering sunglasses in the search bar.\n",
    "sun_glasses = driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "sun_glasses.send_keys(\"sunglasses\")\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()\n",
    "\n",
    "Brand_ = []\n",
    "ProductDescription_ = []\n",
    "Price_ = []\n",
    "Percentageoff_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "799e290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 40 40 40 40\n"
     ]
    }
   ],
   "source": [
    "# scraping Brand Name\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:]:\n",
    "    brand=i.text\n",
    "    Brand_.append(brand)\n",
    "    \n",
    "# scraping ProductDescription\n",
    "productdes_tags=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in productdes_tags[0:]:\n",
    "    product=i.text\n",
    "    ProductDescription_.append(product)\n",
    "    \n",
    "# scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags[0:]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "# scraping Percentage OFF\n",
    "percentageoff_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags[0:]:\n",
    "    percentageoff=i.text\n",
    "    Percentageoff_.append(percentageoff)\n",
    "    \n",
    "    \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51169842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT PAGE STARTS < HERE\n",
    "\n",
    "# searching for the next_page icon bar on the page no 01\n",
    "\n",
    "next_page = driver.find_element(By.CLASS_NAME,\"_1LKTO3\")\n",
    "next_page.send_keys(\"NEXT\")\n",
    "\n",
    "# Click On Search Icon , of page 1.\n",
    "search = driver.find_element(By.CLASS_NAME,\"_1LKTO3\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0c33473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 80 80 80 80\n"
     ]
    }
   ],
   "source": [
    "# scraping Brand Name , of page 2.\n",
    "brand_tags2=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags2[0:]:\n",
    "    brand2=i.text\n",
    "    Brand_.append(brand2)\n",
    "    \n",
    "# scraping ProductDescription  , of page 2.\n",
    "productdes_tags2=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in productdes_tags2[0:]:\n",
    "    product2=i.text\n",
    "    ProductDescription_.append(product2)\n",
    "    \n",
    "# scraping Price  , of page 2.\n",
    "price_tags2=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags2[0:]:\n",
    "    price2=i.text\n",
    "    Price_.append(price2)\n",
    "    \n",
    "    \n",
    "\n",
    "# scraping Percentage OFF  , of page 2.\n",
    "percentageoff_tags2=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags2[0:]:\n",
    "    percentageoff2=i.text\n",
    "    Percentageoff_.append(percentageoff2)\n",
    "    \n",
    "    \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae64304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT PAGE STARTS < HERE\n",
    "\n",
    "# searching for the next_page icon bar on the page no 01\n",
    "\n",
    "next_page2 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]\")\n",
    "next_page2.send_keys(\"NEXT\")\n",
    "\n",
    "# Click On Search Icon , of page 1.\n",
    "search2 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]\")\n",
    "search2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dbf8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scraping Brand Name , of page 2.\n",
    "brand_tags3=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags3[0:20]:\n",
    "    brand3=i.text\n",
    "    Brand_.append(brand3)\n",
    "    \n",
    "# scraping ProductDescription  , of page 2.\n",
    "productdes_tags3=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in productdes_tags3[0:20]:\n",
    "    product3=i.text\n",
    "    ProductDescription_.append(product3)\n",
    "    \n",
    "# scraping Price  , of page 2.\n",
    "price_tags3=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags3[0:20]:\n",
    "    price3=i.text\n",
    "    Price_.append(price3)\n",
    "    \n",
    "    \n",
    "\n",
    "# scraping Percentage OFF  , of page 2.\n",
    "percentageoff_tags3=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags3[0:20]:\n",
    "    percentageoff3=i.text\n",
    "    Percentageoff_.append(percentageoff3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dda788f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_Number</th>\n",
       "      <th>BRAND_Of_Product</th>\n",
       "      <th>PRODUCT_DES_DETAILS</th>\n",
       "      <th>PRICE_DETAILS</th>\n",
       "      <th>PERCENTAGEOFF_DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹521</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (53)</td>\n",
       "      <td>₹278</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>Riding Glasses, Night Vision Spectacle Sunglas...</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹489</td>\n",
       "      <td>38% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection Cat-eye Sunglasses (Free Size)</td>\n",
       "      <td>₹149</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized, UV Protection Wayfarer, Retro Squar...</td>\n",
       "      <td>₹664</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Wayfarer ...</td>\n",
       "      <td>₹899</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection, Mirrored Retro Square Sunglasse...</td>\n",
       "      <td>₹357</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection, Mirrored Round Sunglasses (Free...</td>\n",
       "      <td>₹265</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Serial_Number BRAND_Of_Product  \\\n",
       "0               1   ROZZETTA CRAFT   \n",
       "1               2     Silver Kartz   \n",
       "2               3       LIZA ANGEL   \n",
       "3               4         Fastrack   \n",
       "4               5         Fastrack   \n",
       "..            ...              ...   \n",
       "95             96           SUNBEE   \n",
       "96             97        ROYAL SON   \n",
       "97             98    VINCENT CHASE   \n",
       "98             99       PHENOMENAL   \n",
       "99            100   kingsunglasses   \n",
       "\n",
       "                                  PRODUCT_DES_DETAILS PRICE_DETAILS  \\\n",
       "0   UV Protection, Gradient Retro Square Sunglasse...          ₹521   \n",
       "1            UV Protection Clubmaster Sunglasses (53)          ₹278   \n",
       "2   Riding Glasses, Night Vision Spectacle Sunglas...          ₹129   \n",
       "3    UV Protection Rectangular Sunglasses (Free Size)          ₹489   \n",
       "4       UV Protection Wayfarer Sunglasses (Free Size)          ₹599   \n",
       "..                                                ...           ...   \n",
       "95       UV Protection Cat-eye Sunglasses (Free Size)          ₹149   \n",
       "96  Polarized, UV Protection Wayfarer, Retro Squar...          ₹664   \n",
       "97  by Lenskart Polarized, UV Protection Wayfarer ...          ₹899   \n",
       "98  UV Protection, Mirrored Retro Square Sunglasse...          ₹357   \n",
       "99  UV Protection, Mirrored Round Sunglasses (Free...          ₹265   \n",
       "\n",
       "   PERCENTAGEOFF_DISCOUNT  \n",
       "0                 73% off  \n",
       "1                 81% off  \n",
       "2                 87% off  \n",
       "3                 38% off  \n",
       "4                 40% off  \n",
       "..                    ...  \n",
       "95                85% off  \n",
       "96                66% off  \n",
       "97                55% off  \n",
       "98                82% off  \n",
       "99                84% off  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Serial_Number':range(1,101,1),'BRAND_Of_Product':Brand_,'PRODUCT_DES_DETAILS':ProductDescription_,'PRICE_DETAILS':Price_,'PERCENTAGEOFF_DISCOUNT':Percentageoff_})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984be8e8",
   "metadata": {},
   "source": [
    "# Question no 06"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2903a03f",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sneakers” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sneakers. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sneakers.\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c41559ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3c3a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fe22c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering sunglasses in the search bar.\n",
    "sneakers_ = driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "sneakers_.send_keys(\"sneakers\")\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()\n",
    "\n",
    "Brand_ = []\n",
    "ProductDescription_ = []\n",
    "Price_ = []\n",
    "Percentageoff_ = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "deb56bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 40 0 40 40\n"
     ]
    }
   ],
   "source": [
    "# scraping Brand Name\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:41]:\n",
    "    brand=i.text\n",
    "    Brand_.append(brand)\n",
    "    \n",
    "# scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags[0:41]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "# scraping Percentage OFF\n",
    "percentageoff_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags[0:41]:\n",
    "    percentageoff=i.text\n",
    "    Percentageoff_.append(percentageoff)\n",
    "      \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5c70002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click On Search Icon , of page 1.\n",
    "search = driver.find_element(By.CLASS_NAME,\"_1LKTO3\")\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02d39e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 80 0 80 80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# scraping Brand Name\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:41]:\n",
    "    brand=i.text\n",
    "    Brand_.append(brand)\n",
    "    \n",
    "# scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags[0:41]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "# scraping Percentage OFF\n",
    "percentageoff_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags[0:41]:\n",
    "    percentageoff=i.text\n",
    "    Percentageoff_.append(percentageoff)\n",
    "      \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdbbd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click On Search Icon , of page 1.\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]/span\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21ea6ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 101 0 101 101\n"
     ]
    }
   ],
   "source": [
    "# scraping Brand Name\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:21]:\n",
    "    brand=i.text\n",
    "    Brand_.append(brand)\n",
    "    \n",
    "# scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags[0:21]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "# scraping Percentage OFF\n",
    "percentageoff_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags[0:21]:\n",
    "    percentageoff=i.text\n",
    "    Percentageoff_.append(percentageoff)\n",
    "      \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "617fc551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_Number</th>\n",
       "      <th>BRAND_Of_Product</th>\n",
       "      <th>PRICE_DETAILS</th>\n",
       "      <th>PERCENTAGEOFF_DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Labbin</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Shozie</td>\n",
       "      <td>₹295</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SFR</td>\n",
       "      <td>₹209</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RapidBox</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SFR</td>\n",
       "      <td>₹299</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>U.S. POLO ASSN.</td>\n",
       "      <td>₹2,789</td>\n",
       "      <td>38% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>BIG FOX</td>\n",
       "      <td>₹774</td>\n",
       "      <td>69% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>DEXTURE</td>\n",
       "      <td>₹849</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>₹459</td>\n",
       "      <td>54% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>BIG FOX</td>\n",
       "      <td>₹774</td>\n",
       "      <td>69% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial_Number BRAND_Of_Product PRICE_DETAILS PERCENTAGEOFF_DISCOUNT\n",
       "0                1           Labbin          ₹399                60% off\n",
       "1                2           Shozie          ₹295                70% off\n",
       "2                3              SFR          ₹209                65% off\n",
       "3                4         RapidBox          ₹599                40% off\n",
       "4                5              SFR          ₹299                70% off\n",
       "..             ...              ...           ...                    ...\n",
       "96              97  U.S. POLO ASSN.        ₹2,789                38% off\n",
       "97              98          BIG FOX          ₹774                69% off\n",
       "98              99          DEXTURE          ₹849                73% off\n",
       "99             100     Robbie jones          ₹459                54% off\n",
       "100            101          BIG FOX          ₹774                69% off\n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Serial_Number':range(1,102,1),'BRAND_Of_Product':Brand_,'PRICE_DETAILS':Price_,'PERCENTAGEOFF_DISCOUNT':Percentageoff_})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24f856",
   "metadata": {},
   "source": [
    "# Question no 07"
   ]
  },
  {
   "cell_type": "raw",
   "id": "184b1b2e",
   "metadata": {},
   "source": [
    "Q7: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b44f45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the amazon page on automated chrome browser\n",
    "driver.get(\"https://www.amazon.in/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1ba2b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering \"Laptop\" in the search bar.\n",
    "laptop_ = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "laptop_.send_keys(\"Laptop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92a52a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search.click()\n",
    "\n",
    "Title_ = []\n",
    "Rating_ = []\n",
    "Price_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9b4d9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering sunglasses in the search bar.\n",
    "cpu_type = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[6]/li[11]/span/a/span\")\n",
    "cpu_type.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "559dcff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lenovo IdeaPad 3 11th Gen Intel Core i3 15.6\" FHD Thin & Light Laptop(8GB/512GB SSD/Windows 11/Office 2021/2Yr Warranty/3months Xbox Game Pass/Platinum Grey/1.7Kg), 81X800LGIN', 'HP 15s 11th Gen Intel Core i3 8GB RAM/512GB SSD 15.6inches/39.6cm FHD, Anti-Glare, Micro-Edge Display/Intel UHD Graphics/Dual Speakers/Windows 11/Alexa Built-in/MSO 2021/1.69kg, 15s-fq2671TU', 'Acer Extensa 15 Lightweight Laptop Intel Core i3 11th Gen Processor (4 GB RAM/256GB SSD/Windows 11 Home/Intel UHD Graphics /1.7Kg/Black) EX215-54 with 15.6\" (39.6 cms) Full HD Display', 'HP 15s,11th Gen Intel Core i3-1115G4 8GB RAM/512GB SSD 15.6-inch(39.6 cm) Micro-Edge Anti-Glare FHD Laptop/Alexa Built-in/Win 11/Intel UHD Graphics/Dual Speakers/ MS Office 2021/1.69 Kg, 15s-fq2673TU', 'ASUS Vivobook 14, 14.0-inch (35.56 cms) FHD, Intel Core i3-1005G1 10th Gen, Thin and Light Laptop (8GB/512GB SSD/Integrated Graphics/Windows 11/Office 2021/Grey/1.60 kg), X415JA-EK324WS', 'Lenovo IdeaPad Slim 3 Intel Core i3 10th Gen 15.6\" (39.62cm) FHD Thin & Light Laptop (8GB/1TB HDD/Windows 11/Office 2021/2Yr Warranty/3months Game Pass/Platinum Grey/1.7Kg), 81WB01E7IN', 'Lenovo IdeaPad Slim 3 Intel Core i3 11th Gen 14\" (35.56cm) FHD Thin & Light Laptop (8GB/256GB SDD/Windows 11/Office 2021/2Yr Warranty/3months Game Pass/Platinum Grey/1.5Kg), 81X700CWIN', 'HP 14s, 11th Gen Intel Core i3-1115G4, 8GB RAM/256GB SSD 14-inch(35.6 cm) Micro-Edge, Anti-Glare, FHD Laptop/Alexa Built-in/Win 11/Intel UHD Graphics/Dual Speakers/ MSO 2021/1.41 Kg, 14s-dy2507TU', 'Dell Vostro 3420 Laptop - Intel Core i3-1115G4, 8GB DDR4, 512GB SSD, Win 11 + MSO\\'21, 14.0\"/35.56Cms, FHD WVA AG 250 nits, Carbon Black (D552276WIN9BE, 1.48Kgs)', 'Lenovo IdeaPad 3 11th Gen Intel Core i3 15.6\" FHD Thin & Light Laptop(8GB/512GB SSD/Windows 11/Office 2021/2Yr Warranty/3months Xbox Game Pass/Platinum Grey/1.7Kg), 81X800LGIN', 'Dell Inspiron 3511 Laptop, Intel Core i3-1115G4, 8GB, 512GB SSD, Win 11 + MSO, 15.6\" (39.62Cms) FHD WVA AG Narrow Border, Carbon Black (D560842WIN9B, 1.8Kgs)']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# scraping Brand Name\n",
    "title_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in title_tags[0:11]:\n",
    "    title=i.text\n",
    "    Title_.append(title)\n",
    "print(Title_)\n",
    "print(len(Title_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c62904aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# scraping ProductDescription\n",
    "ratings_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-size-base\"]')\n",
    "for i in ratings_tags[0:11]:\n",
    "    ratings=i.text\n",
    "    Rating_.append(ratings)\n",
    "print(Rating_)\n",
    "print(len(Rating_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12316c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['38,490', '40,150', '29,990', '43,490', '32,820', '', '', '', '', '', '']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in price_tags[0:11]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "print( Price_)\n",
    "print(len(Price_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f344c7ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12348\\740802912.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Serial_Number'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'TITLE_Of_Product'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mTitle_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'RATING_OF_PRODUCT'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mRating_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PRICE_DETAILS'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mPrice_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All arrays must be of the same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame({'Serial_Number':range(1,11,1),'TITLE_Of_Product':Title_[0:10],'RATING_OF_PRODUCT':Rating_[0:10],'PRICE_DETAILS':Price_[0:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f25ba82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mam, the Amazon Website is very worse, i tried it many a times on the same code . it gives sometimes out as we requires\n",
    "#and some time with same codes it is not working.\n",
    "#mam, my code is correct but , web page issue .\n",
    "\n",
    "# mam Please find seperate github page link for question no 07 , it is creating issues .Here is creating malfunctioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1ecdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
